# -*- coding: utf-8 -*-
"""NLP with Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pEX3SZaJ3vZqgDHP0AMbfw2DbSgu0rFK
"""

# Data Diri
"""
Tria Agusti Khoirun Nisa'
nisaagustin6@gmail.com
Watesprojo, Kemla, Mojokerto
Proyek Pertama : Membuat Model NLP dengan TensorFlow
"""

"""Mengimpor library yang dibutuhkan 

"""

import pandas as pd
import tensorflow as tf
import csv
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

"""Membaca dataset yang sudah diupload"""

df = pd.read_csv('spam.csv')
df

"""Melihat informasi yang terkandung di dalam dataset menggunakan fungsi df.info()"""

df.info()

"""Karena dataset merupakan data kategorikal, maka perlu melakukan proses one-hot-encoding"""

category = pd.get_dummies(df.Category)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Category')
df_baru

"""Menghapus teks dan tanda baca yang tidak diperlukan dalam dataset"""

STOPWORDS = set(stopwords.words('english'))
articles = []
labels = []

with open("spam.csv", 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    for row in reader:
        labels.append(row[0])
        article = row[1]
        for word in STOPWORDS:
            token = ' ' + word + ' '
            article = article.replace(token, ' ')
            article = article.replace(' ', ' ')
        articles.append(article)

"""Mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values"""

message = df_baru['Message'].values
label = df_baru[['ham', 'spam']].values

"""Membagi dataset untuk data training dan data testing dengan validation set 20%"""

from sklearn.model_selection import train_test_split
message_train, message_test, label_train, label_test = train_test_split(message, label, test_size=0.2)

"""Mengubah setiap kata pada dataset ke dalam bilangan numerik dengan fungsi Tokenizer. Kemudian, membuat konversi untuk setiap sampel menjadi sequence"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(message_train) 
tokenizer.fit_on_texts(message_test)
 
sekuens_train = tokenizer.texts_to_sequences(message_train)
sekuens_test = tokenizer.texts_to_sequences(message_test)

padded_train = pad_sequences(sekuens_train,
                            padding='post',
                            maxlen=1500,
                            truncating='post')
padded_test = pad_sequences(sekuens_test,
                            padding='post',
                            maxlen=1500,
                            truncating='post')

"""Menerapkan Embedding dengan menggunakan dimensi embbeding sebesar 16, serta dimensi iinput sebesar nilai num_words pada objek tokonizer. 
Kemudian, memanggil fungsi compile dan menentukan optimizer serta loss function yang akan dipakai oleh model
"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

print(model.summary())

"""Menerapkan callback pada model """

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

"""Melatih model dengan memanggil fungsi fit()"""

num_epochs = 15
history = model.fit(padded_train, label_train, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2)