# -*- coding: utf-8 -*-
"""Machine Learning with Data Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_8evVeHfyo4y9byJKnQ0IpRSrTbG4eA4
"""

# Data Diri
"""
Tria Agusti Khoirun Nisa'
nisaagustin6@gmail.com
Watesprojo, Kemlagi, Mojokerto
Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series
"""

"""Mengimpor library yang dibutuhkan pada Google Colab"""

import pandas as pd
import tensorflow as tf
import numpy as np

from keras.layers import Dense, LSTM

import matplotlib.pyplot as plt

"""Mengubah dataset menjadi dataframe dengan fungsi read_csv(). Kemudian menampilkan data pada dataframe """

df = pd.read_csv('Microsoft_Stock.csv')
df

df.info()

"""Mengecek apakah ada nilai yang hilang dari dataset"""

df.isnull().sum()

"""Membuat plot dari dataset dapat menggunakan fungsi plot dari libabry matplotlib"""

dates = df['Date'].values
volume = df['Volume'].values

plt.figure(figsize=(15,5))
plt.plot(dates, volume)
plt.title('Volume Average',
          fontsize=20)

"""Melakukan proses normalisasi (*normalization*) pada dataset sebelum digunakan untuk melatih model"""

from sklearn.preprocessing import MinMaxScaler
volume = np.array(df['Volume'])
volume = volume.reshape(-1, 1)

scaler = MinMaxScaler()
volume = scaler.fit_transform(volume)
volume

"""Pembagian dataset menjadi data latih dan data vakidation"""

dates = np.array(df['Volume'].values, dtype=np.float)
vol = df['Volume'].values

"""Menggunakan train_test_split dan melakukan validation set sebesar 20%. Hal ini dilakukan untuk memastikan pembagian data yang Anda lakukan konsisten dan tidak acak."""

from sklearn.model_selection import train_test_split

dates_train, dates_val, volume_train, volume_val = train_test_split(dates, volume, train_size=0.8, test_size = 0.2, shuffle = False)

print('Total Data Train : ',len(volume_train))
print('Total Data Validation : ',len(volume_val))

"""Menambahkan fungsi untuk menerima sebuah series atau tribut yang telah dikonversi menjadi tipe numpy. Lalu mengembalikan label dan atribut dari datasetdalam bentuk batch. """

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

"""Arsitektur model menggunakan 2 buah layer LSTM"""

tf.keras.backend.set_floatx('float64')

train_set = windowed_dataset(volume_train, window_size=64, batch_size=200, shuffle_buffer=1000)
val_set = windowed_dataset(volume_val, window_size=64, batch_size=200, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True, input_shape = [None, 1]),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

"""Menggunakan perintah untuk mengetahui ambang batas (threshold) yang tepat untuk data yang digunakan"""

mae = (volume.max() - volume.min()) * 10/100
mae

"""Pemanggilan fungsi Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.10):
      print("\nMae telah mencapai <10%!")
      self.model.stop_training = True
callbacks = myCallback()

"""Melakukan pelatihan model"""

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=50, validation_data = val_set, callbacks=[callbacks])